# Privacy Policy Parser Code

This directory contains the core implementation for extracting and downloading privacy policy documents from websites. The system uses a two-step approach: first extracting potential privacy policy links from a website, then downloading the actual privacy policy documents.


## Files Overview

### `website_parser.py`
A web scraper that extracts potential privacy policy links from websites using both HTTP requests and Selenium WebDriver.

**Usage:**
```bash
# Basic usage
python website_parser.py <url>

# With custom configuration
python website_parser.py <url> --config config.json --output results.json
```

**Output:** Creates `privacy_policy_links.json` with extracted links

### `pp_download.py`
Downloads and saves privacy policy documents using the polipy library.

**Usage:**
```bash
# Basic usage
python pp_download.py

# With custom settings
python pp_download.py --input links.json --config config.json --output-dir ./policies
```

**Dependencies:**
- Requires `privacy_policy_links.json` file (generated by website_parser.py)

## System Architecture

```
Website URL → website_parser.py → privacy_policy_links.json → pp_download.py → Privacy Policy Documents
```

1. **Link Extraction Phase:**
   - Input: Website URL
   - Process: Parse HTML, identify privacy-related links using heuristics
   - Output: JSON file with potential privacy policy links

2. **Document Download Phase:**
   - Input: JSON file with privacy policy links
   - Process: Download and parse each privacy policy using polipy
   - Output: Privacy policy documents saved to `./new_pp/` directory


## Configuration

### Firefox WebDriver Setup
The code is configured to use a specific Firefox installation:
- Binary location: `/localtemp/fr3ya/projects/GDPR/data_collection/website_pp_parser/firefox/firefox`
- Geckodriver: `/u/fr3ya/miniforge3/bin/geckodriver`
- Headless mode enabled
- Custom user agent for better compatibility

### Proxy Configuration
- HTTP/HTTPS proxy: `http://127.0.0.1:3128`
- Can be modified in the `WebsiteParser.__init__()` method




## Output Format

### privacy_policy_links.json
```json
{
    "url": "https://example.com",
    "potential_privacy_policy_links": [
        "https://example.com/privacy",
        "/privacy-policy",
        "https://example.com/terms"
    ]
}
```

### Downloaded Policies
- Saved to `./new_pp/` directory
- Includes screenshots if enabled
- Organized by source URL

## Quick Start

### Prerequisites
```bash
# Install required dependencies
pip install requests beautifulsoup4 selenium polipy

# Make sure you have Firefox and geckodriver installed
# Update paths in config_example.json if needed
```

### Run the Parser
```bash
# Step 1: Extract privacy policy links from a website
python website_parser.py https://example.com

# Step 2: Download the privacy policy documents
python pp_download.py
```

That's it! The privacy policy documents will be saved in the `./new_pp` directory.

### Example Run
```bash
# Example: Extract privacy policy from Google
python website_parser.py https://google.com
# Output: privacy_policy_links.json

python pp_download.py
# Output: Privacy policy documents in ./new_pp/
```



## Usage Guide

### 1. Basic Usage

#### Extract Privacy Policy Links
```bash
# Use default configuration
python website_parser.py https://example.com

# Specify output file
python website_parser.py https://example.com --output results.json
```

#### Download Privacy Policy Documents
```bash
# Use default configuration
python pp_download.py

# Specify input file and output directory
python pp_download.py --input results.json --output-dir ./policies
```

### 2. Using Configuration Files

#### Create Configuration File
```bash
# Copy example configuration file
cp config_example.json my_config.json

# Edit configuration file
nano my_config.json
```

#### Use Configuration File
```bash
# Extract links with configuration file
python website_parser.py https://example.com --config my_config.json

# Download with configuration file
python pp_download.py --config my_config.json
```



### 4. Configuration File Reference

#### Complete Configuration Example
```json
{
  "webdriver": {
    "firefox_binary_path": "/path/to/firefox",
    "geckodriver_path": "/path/to/geckodriver", 
    "headless": true,
    "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
    "wait_time": 30
  },
  "proxy": {
    "enabled": true,
    "http_proxy": "http://127.0.0.1:3128",
    "https_proxy": "http://127.0.0.1:3128"
  },
  "parser": {
    "timeout": 20,
    "max_tokens": 6,
    "popup_max_tokens": 4,
    "enable_translation": false,
    "retry_attempts": 2
  },
  "output": {
    "output_dir": "./new_pp",
    "links_file": "privacy_policy_links.json",
    "enable_screenshots": true,
    "save_raw_html": false
  }
}
```

#### Configuration Parameters

**WebDriver Configuration:**
- `firefox_binary_path`: Path to Firefox browser executable
- `geckodriver_path`: Path to Geckodriver executable
- `headless`: Whether to run in headless mode
- `user_agent`: User agent string for requests
- `wait_time`: Page load wait time in seconds

**Proxy Configuration:**
- `enabled`: Whether to enable proxy
- `http_proxy`: HTTP proxy address
- `https_proxy`: HTTPS proxy address

**Parser Configuration:**
- `timeout`: HTTP request timeout in seconds
- `max_tokens`: Maximum token count for privacy link text
- `popup_max_tokens`: Maximum token count for popup privacy link text
- `enable_translation`: Whether to enable translation for non-English sites
- `retry_attempts`: Number of retry attempts

**Output Configuration:**
- `output_dir`: Output directory for downloaded policies
- `links_file`: Filename for extracted links JSON
- `enable_screenshots`: Whether to save screenshots
- `save_raw_html`: Whether to save raw HTML content


